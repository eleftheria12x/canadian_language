---
title: "project2"
author: "Anastasia Hatjiyianni"
date: "2024-11-08"
output: html_document
---

**Exercise 2**

```{r}
library(TSA)
data('deere3')
plot(deere3,type='o',main='Deere3 Data')
```

(a) Estimate the parameters of an AR(1) model for this series. Discuss any diagnostics that
might be useful.

```{r}
fit_ar1<-arima(deere3,order=c(1,0,0))
fit_ar1
```

```{r}
#plots of the residuals
acf(fit_ar1$residuals,na.action=na.pass,main='ACF of residuals for AR(1)')
pacf(fit_ar1$residuals,na.action=na.pass,main='PACF of residuals for AR(1)')
```

```{r}
library(forecast)
checkresiduals(fit_ar1)
```

(b) Estimate the parameters of an AR(2) model for this series and discuss again diagnostics.

```{r}
fit_ar2<-arima(deere3,order=c(2,0,0))
fit_ar2
```

```{r}
#plots of the residuals
acf(fit_ar2$residuals,na.action=na.pass,main='ACF of residuals for AR(2)')
pacf(fit_ar2$residuals,na.action=na.pass,main='PACF of residuals for AR(2)')
```

```{r}
library(forecast)
checkresiduals(fit_ar2)
```
(c) The residual diagnostics show that both AR(1) and AR(2) models have nearly identical residuals suggesting that they both capture the underlying data patterns similarly well.In both models residuals appeared to be uncorrelated and they are within the dash lines indicating that they might resemble white noise. Additionally, the residual variances are almost identical, and the residuals seem to follow a normal distribution.

Both models have very similar coefficients for φ1. Specifically the AR(1) model is 0.5255 while the AR(2) coefficient 0.5211 which are almost the same .
The second coefficient of AR(2) model φ2 is 0.0083 which is very close to zero. .Also, it is not statistically significant, as the p-value of its t-statistic is 0.949, which is far higher than the typical significance level (e.g., 0.05). This indicates that the second term in the AR(2) model does not meaningfully contribute to explaining the data.

In combination with the fact that the AIC of the AR(1) model (995.02) is lower than that of the AR(2) model (997.01), it is clear that the AR(2) model does not improve upon the AR(1) model’s performance. Therefore, the AR(1) model is the better choice, as it provides a simpler, equally effective fit without the unnecessary complexity of the additional coefficient.



```{r}
ar2_coef <- fit_ar2$coef
ar2_se <- sqrt(diag(fit_ar2$var.coef))  # standard errors

ar2_t_values <- ar2_coef / ar2_se
ar2_p_values <- 2 * (1 - pnorm(abs(ar2_t_values)))
ar2_p_values
```

**Exercise 3**
Simulate an AR(1) process with ϕ = 0.8 and m = 100. Simulate 208 values but
set aside the last 8 values to compare forecasts to actual values.
```{r}
phi<-0.8
m<-100
n<-208
```

(a) Using the first 200 values forecast the next eight values of the series. Plot the series
together with the eight forecasts. Place a horizontal line at the estimate of the process
mean.

```{r}
set.seed(13)
X<-arima.sim(model=list(ar=phi),n=n)+m
train<-window(X,1,200)
test<-window(X,201,208)

library(forecast)
fit.ar1<-arima(train,order = c(1,0,0))
plot(forecast(fit.ar1,h=8,level=95),main='Forecast for AR(1)')
lines(seq(201,208,1),test,lwd=2,col='red')
abline(h=mean(train),lty=3)

legend("topleft", 
       legend = c("Real data", "Prediction intervals", 
                  "Training mean", "Forecast values"),
       col = c("red", "grey", "black", "blue"),
       lty = c(1, NA, 3, 1), 
       lwd = c(2, NA, 1, 1),
       pch = c(NA, 15, NA, NA), # Filled square for the grey prediction intervals
       pt.cex = 2, # Makes the grey square larger for visibility
       bty = "n")
```
b) The comparison is shown in the plot, where the forecast values are displayed in blue and the actual values in red. You can zoom in on the plot for a clearer view. Additionally, a dataframe has been created for easier comparison.

Zoom the plot
```{r}
plot(forecast(fit.ar1,h=8,level=95), main = "Forecast for AR(1)", xlim = c(195, 208))

# Add actual test values in red
lines(seq(201, 208, 1), test, lwd = 2, col = 'red')

# Add mean line of the training data
abline(h = mean(train), lty = 3)

# Add legend
legend("topleft", 
       legend = c("Real data", "Prediction intervals", 
                  "Training mean", "Forecast values"),
       col = c("red", "grey", "black", "blue"),
       lty = c(1, NA, 3, 1), 
       lwd = c(2, NA, 1, 1),
       pch = c(NA, 15, NA, NA), # Filled square for the grey prediction intervals
       pt.cex = 2, # Makes the grey square larger for visibility
       bty = "n")
```
Creation of dataframe
```{r}
comparison <- data.frame(
    Time = 201:208, Actual = test,
    Forecast = forecast(fit.ar1, h=8)$mean,
    Lower_95 = forecast(fit.ar1, h = 8)$lower[, 2],
    Upper_95 = forecast(fit.ar1, h = 8)$upper[, 2]
   )
comparison
```

c) The 95% forecast limits are shown in grey on the plot. As we can see from both the plot and the data frame, all the actual values fall within the forecast limits.

**Exercise 4**
```{r}
library(TSA)
data('boardings')
boardings <- boardings[, -ncol(boardings)]
```

a)Produce the time series plot for these data. Be sure to use plotting symbols that will
help you assess seasonality. Does a stationary model seem reasonable?

```{r}
#boardings <- ts(boardings, frequency=12)
layout(matrix(c(1,1,2, 1,1,3), nc=2))
#par(mar=c(3,3,2,1), mgp=c(1.6,.6,0))

plot(boardings, pch = 16, main = "Time Series Plot of Monthly Boardings",
     xlab = "Time (Months)", ylab = "Number of Boardings")
Months = c("A","S","O","N","D","J","F","M","A","M","J","J")
points(boardings,pch=Months, cex=1.25, font=4, col=1:4)
#axis(1, at=seq(2000, 2005, by=1)); 
#abline(v=seq(2000, 2005, by=1), lty=2, col=gray(.7))
#axis(2)
#box()
```

The log boardings time series shows a clear seasonal pattern, with recurring peaks and troughs in specific months. For example, there is a peak in September and October, while December shows a trough. This suggests that ridership follows a regular cycle each year.
Also this time series show a slightly upward trend since there is an overall increase in the of log.boardings over time.
Given that the time series exhibits seasonality and a slight upward trend,a stationary model is not seems reasonable.



## (B)Calculate and plot the sample ACF for this series. At which lags do you have significant autocorrelation?

```{r}
acf(boardings, main="ACF of log.boardings", lag.max = 24)
```


The ACF plot for the `log.boardings` data reveals a clear seasonal pattern, with each lag representing one month and showing correlations over the first two years. We observe significant positive autocorrelations at lag 0.1, indicating short-term dependencies, and at lag 1.0, suggesting a strong annual seasonality, as values from the same month in consecutive years are correlated. Additionally, there are significant autocorrelations around lags 0.4, 0.5, and 0.6, indicating that certain months within each year are closely correlated. The autocorrelation gradually decreases with increasing lag, but the recurring peaks around 12-month intervals reinforce the presence of annual cycles.

## (C)Fit an ARMA(0, 3)×(1, 0)12 model to these data. Assess the significance of the estimated coefficients.

```{r}
library(astsa)
fit1<-sarima(boardings,0,0,3,1,0,0,12)
```
All the estimated coefficients—the three moving average terms, the seasonal autoregressive term, and the mean value—are significant since their p value is lower than the level of significance.

## (d)Fit now an ARMA(0, 4)×(1, 0)12 model–that is a model with an additional parameter. Interpret the results.

```{r}
library(astsa)
fit2<-sarima(boardings,0,0,4,1,0,0,12)
```
In the ARMA(0, 4)×(1, 0)12 model, the first three moving average terms (ma1, ma2, and ma3) the seasonal autoregressive term(sar1) and the mean value are significant similarly to the ARMA(0, 3)×(1, 0)12 model. However the fourth moving average term which added now is not significant (as the p-value is higher than 0.05) indicating it does not improve the model's performance. Since both models achieve similar AIC values, with the ARMA(0, 3)×(1, 0)12 model having a slightly lower AIC, the simpler model is preferred for its simplicity.


##New anastasia:

```{r}
# Load necessary library
library(forecast)
 
# Parameters for the AR(1) process
phi <- 0.8
mean_val <- 100
n_total <- 208   # Total number of simulated values
n_forecast <- 8  # Number of forecast values
n_train <- n_total - n_forecast  # Number of values for training
 
# Simulate AR(1) process with phi = 0.8 and mean = 100
set.seed(0)  # For reproducibility
ar_process <- arima.sim(model = list(ar = phi), n = n_total) + mean_val
 
# Check if the simulation generated the expected number of values
if (length(ar_process) < n_total) {
  stop("Simulation error: Not enough observations in the generated series.")
}
 
# (a) Forecast the next 8 values
train_series <- window(ar_process, end = n_train)  # Use first 200 values for training
fit <- Arima(train_series, order = c(1, 0, 0), include.mean = TRUE)  # Fit AR(1) model
forecasts <- forecast(fit, h = n_forecast)  # Forecast next 8 values
 
# Plot the series together with the eight forecasts and mean line
plot(forecasts, main = "AR(1) Process with Forecasts", ylab = "Value", xlab = "Time")
lines(ar_process, col = "blue")  # Actual series
abline(h = mean_val, col = "red", lty = 2)  # Horizontal line at process mean
legend("topleft", legend = c("Actual Series", "Forecasted Series", "Process Mean"),
       col = c("blue", "black", "red"), lty = c(1,1,2))
 
# (b) Compare the eight forecasts with the actual values
actual_values <- window(ar_process, start = n_train + 1)  # Last 8 actual values
forecast_values <- forecasts$mean  # Forecasted values
comparison <- data.frame(Time = (n_train + 1):n_total,
                         Actual = actual_values, Forecast = forecast_values)
print("Comparison of Actual and Forecasted Values:")
print(comparison)
 
# (c) Plot forecasts with 95% forecast intervals and check if actuals fall within limits
plot(forecasts, main = "Forecasts with 95% Confidence Intervals")
lines(actual_values, col = "blue", lty = 2)  # Actual values for comparison
abline(h = mean_val, col = "red", lty = 2)  # Mean line
legend("topleft", legend = c("Forecasted Series", "95% Confidence Interval", "Actual Values"),
       col = c("black", "gray", "blue"), lty = c(1,1,2))
 
# Check if actual values fall within 95% confidence intervals
within_limits <- actual_values >= forecasts$lower[,2] & actual_values <= forecasts$upper[,2]
result <- data.frame(Time = (n_train + 1):n_total,
                     Actual = actual_values,
                     Lower_Bound = forecasts$lower[,2],
                     Upper_Bound = forecasts$upper[,2],
                     Within_Limits = within_limits)
print("Actual values within 95% forecast limits:")
print(result)

```
